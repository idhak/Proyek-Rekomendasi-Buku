# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bV_OaOx6VMAgL9g9BvFgG1xerJ23mqfl

**Nama** : Idha Kurniawati

**Email** : idhakurniawati03@gmail.com

**ID Dicoding** : idhakt
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
arashnic_book_recommendation_dataset_path = kagglehub.dataset_download('arashnic/book-recommendation-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

#import numpy as np # linear algebra
#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""## Project Overview

Buku adalah jendela dunia. Salah satu cara untuk mendapatkan informasi dan mempelajari sesuatu adalah dengan membaca buku. Buku bukan sekedar kumpulan data, melainkan jalan menuju perubahan. Setiap aspek kehidupan manusia dipengaruhi oleh kemajuan teknologi. Sekarang buku tidak lagi hanya dijual secara offline di toko fisik. Perkembangan teknologi telah memungkinkan buku dijual di banyak toko online melalui platform e-commerce, media sosial, dan situs web pribadi toko buku. Selain itu, buku juga tersedia dalam bentuk e-book. E-book adalah versi digital dari buku cetak dalam format tertentu (misalnya PDF, EPUB atau MOBI) yang memungkinkan pembaca untuk mengakses dan membaca konten buku menggunakan perangkat digital mereka.

UNESCO menyatakan bahwa indeks minat baca masyarakat Indonesia hanya 0,001% atau dari seribu orang Indonesia.  Dalam laman resmi Kementerian Komunikasi dan Informatika Republik Indonesia (Kemenkominfo) juga mengumumkan hasil penelitian yang dilakukan oleh Central Connecticut State University pada Maret 2016 lalu. Hasilnya menunjukkan bahwa Indonesia menduduki peringkat ke-60 dari 61 negara dalam hal minat membaca, tepat di bawah Thailand (59) dan di atas Bostwana (61).  Padahal, Indonesia berada di atas negara-negara Eropa dalam hal penilaian infrastruktur yang mendukung kegiatan membaca. Sementara itu, Programme for International Student Assessment (PISA), sebuah penelitian internasional yang menilai kualitas sistem pendidikan dengan mengukur hasil belajar yang penting untuk keberhasilan di abad ke-21, melaporkan bahwa hasil literasi membaca Indonesia meningkat 5 posisi dibandingkan tahun 2018. Akibatnya, peringkat tersebut menurun, dan Indonesia masih berada di peringkat ke-11 terbawah dari 81 negara yang disurvei.

Hal ini akan mempengaruhi pemahaman teknologi dan sains. Lebih jauh lagi, minimnya kegiatan membaca terutama di kalangan anak muda dapat berdampak buruk bagi negara karena kehilangan sumber daya manusia yang produktif dan berkualitas. Oleh karena itu, untuk mencetak generasi yang cerdas dan produktif, perlu dilakukan upaya peningkatan minat baca di Indonesia. Menyediakan berbagai jenis bahan bacaan untuk mendukung pembelajaran dan mendorong masyarakat agar mencintai buku merupakan salah satu cara untuk meningkatkan minat baca masyarakat. Langkah lain yang dapat dilakukan untuk meningkatkan minat membaca di Indonesia yaitu dengan menggunakan sistem rekomendasi. Sistem rekomendasi memungkinkan pembaca menemukan referensi baru untuk buku yang sesuai dengan kategori yang disukai oleh pembaca sebelumnya. Beberapa sistem rekomendasi terdiri dari  Content-Based Filtering dan Collaborative Filtering. Content-Based Filtering adalah metode yang memberikan saran kepada pengguna berdasarkan preferensi pengguna dan hubungan antara metadata suatu item. Collaborative Filtering adalah model yang memberikan rekomendasi berdasarkan kumpulan pendapat dan minat pengguna, biasanya diberikan oleh pengguna dalam bentuk penilaian pada suatu item. Dengan mengukur kemiripan dari setiap buku, sistem dapat membuat skor untuk membuat rekomendasi yang sesuai dengan metadata buku tersebut. Melalui sistem rekomendasi ini, pembaca diharapkan dapat menambahkan referensi baru berdasarkan buku yang diminati sebelumnya.

Tujuan proyek ini adalah untuk mengembangkan sistem rekomendasi buku yang efektif menggunakan Book Recommendation Dataset dari Kaggle. Dengan menerapkan Content-Based Filtering dan Collaborative Filtering, proyek ini bertujuan untuk memberikan rekomendasi buku yang relevan dengan preferensipengguna, meningkatkan pengalaman membaca, dan berpotensi meningkatkan penjualan dan keterlibatan pada platform buku digital.

**Sumber Referensi**:

- Ardiansyah, R., Ari Bianto, M., & Saputra, B. D. (2023). Sistem Rekomendasi Buku Perpustakaan Sekolah menggunakan Metode Content-Based Filtering. Jurnal CoSciTech (Computer Science and Information Technology), 4(2), 510–518. https://doi.org/10.37859/coscitech.v4i2.5131

- Rosita, A., Puspitasari, N., & Kamila, V. Z. (2022). Rekomendasi Buku Perpustakaan Kampus Dengan Metode Item-Based Collaborative Filtering. Sebatik, 26(1), 340–346. https://doi.org/10.46984/sebatik.v26i1.1551

- Zayyad, M. R. (2021). Sistem Rekomendasi Buku Menggunakan Metode Content-Based Filtering. 1–45.

## Business Understanding

### Problem Statements

Berdasarkan latar belakang di atas, berikut rumusan masalah yang akan diselesaikan dalam proyek ini:
- Bagaimana cara mengembangkan sistem rekomendasi buku yang dapat memberikan saran personal kepada pengguna berdasarkan preferensi?
- Bagaimana mengintegrasikan informasi pada buku untuk memberikan rekomendasi yang akurat?
- Langkah-langkah evaluasi seperti apa yang diperlukan untuk memahami seberapa efektif dua pendekatan sistem rekomendasi yang telah diimplementasikan, serta untuk mengidentifikasi kekuatan dan kelemahan masing-masing?

### Goals

Tujuan dari proyek ini adalah sebagai berikut.:
- Mengembangkan sistem rekomendasi buku yang dapat menyarankan buku-buku relevan sesuai dengan preferensi pengguna secara personal.
- Membangun model yang dapat mengidentifikasi kemiripan antar buku berdasarkan metadata buku.
- Mengevaluasi dan membandingkan performa dua pendekatan sistem rekomendasi yang telah diimplementasikan untuk mengidentifikasi efektivitas, kekuatan, dan kelemahan masing-masing.

### Solution statements

- Pendekatan Content Based Filtering adalah mempelajari profil minat pengguna baru berdasarkan data penilaian objek sebelumnya untuk merekomendasikan item serupa yang disukai di masa lalu atau saat ini dilihat, di mana akurasi rekomendasi meningkat seiring dengan bertambahnya informasi pengguna. Berikut implementasi Content Based Filtering.
    - Mengekstrak fitur dari data buku menggunakan TF-IDF Vectorizer.
    - Menghitung similarity matrix menggunakan cosine similarity.
    - Membuat fungsi rekomendasi yang dapat menyarankan buku serupa berdasarkan judul buku yang diinput.
- Pendekatan Collaborative Filtering adalah merekomendasikan sesuatu berdasarkan perilaku pengguna lain yang memiliki preferensi serupa.  Berikut implementasi Collaborative Filtering.
    - Melatih model untuk memprediksi rating yang mungkin diberikan pengguna terhadap buku yang belum mereka baca.
    - Membuat fungsi rekomendasi yang dapat menyarankan buku-buku dengan prediksi rating tertinggi untuk pengguna tertentu.

Kedua pendekatan ini akan diimplementasikan secara paralel dan dievaluasi untuk menentukan kelebihan dan kekurangan masing-masing dalam konteks rekomendasi buku.

## Data Understanding

### Sumber Data

Dataset yang digunakan dalam proyek ini adalah "Book Recommendation Dataset" yang tersedia di platform Kaggle. Dataset ini dapat diakses melalui link berikut: [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data).

Dataset ini terdiri dari tiga file CSV utama:
1. Books.csv - Informasi tentang buku
2. Ratings.csv - Rating yang diberikan oleh pengguna
3. Users.csv - Informasi tentang pengguna

### Informasi Dataset

Berikut ringkasan informasi mengenai tiga dataset yang dipakai:

**Books Dataset:**
- Jumlah baris: 271360 baris
- Jumlah kolom: 8 kolom

**Ratings Dataset:**
- Jumlah data: 1149780 baris
- Jumlah kolom: 3 kolom

**Users Dataset:**
- Jumlah baris: 278858 baris
- Jumlah kolom: 3 kolom

### Variabel pada Dataset

**1. Books.csv:**
- **ISBN**: International Standard Book Number, kode unik identifikasi buku
- **Book-Title**: Judul buku
- **Book-Author**: Nama penulis buku
- **Year-Of-Publication**: Tahun publikasi buku
- **Publisher**: Nama penerbit buku
- **Image-URL-S**: URL untuk gambar sampul buku ukuran kecil
- **Image-URL-M**: URL untuk gambar sampul buku ukuran medium
- **Image-URL-L**: URL untuk gambar sampul buku ukuran besar

**2. Ratings.csv:**
- **User-ID**: ID unik pengguna
- **ISBN**: ISBN buku yang diberi rating
- **Book-Rating**: Rating yang diberikan oleh pengguna (skala 1-10)

**3. Users.csv:**
- **User-ID**: ID unik pengguna
- **Location**: Lokasi pengguna
- **Age**: Usia pengguna

### Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import re
import tensorflow as tf

from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam, AdamW
from tensorflow.keras.regularizers import l2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import collections

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

"""### Loading Dataset"""

books = pd.read_csv('/kaggle/input/book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('/kaggle/input/book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('/kaggle/input/book-recommendation-dataset/Users.csv')

# Menampilkan 5 data teratas pada dataset Books
print("\nLima data teratas dari dataset Books:")
books.head()

# Menampilkan 5 data teratas pada dataset Users
print("\nLima data teratas dari dataset Users:")
users.head()

# Menampilkan 5 data teratas pada dataset Ratings
print("\nLima data teratas dari dataset Ratings:")
ratings.head()

# Menampilkan jumlah baris dan kolom pada dataset Books
print("Jumlah data pada dataset Books (baris, kolom):")
print(books.shape)

# Menampilkan jumlah baris dan kolom pada dataset Ratings
print("==================================================")
print("\nJumlah data pada dataset Ratings (baris, kolom):")
print(ratings.shape)

# Menampilkan jumlah baris dan kolom pada dataset Users
print("==================================================")
print("\nJumlah data pada dataset Users (baris, kolom):")
print(users.shape)

# Menampilkan informasi struktur dataset Books
print("Informasi dataset Books:")
books.info()

# Menampilkan informasi struktur dataset Ratings
print("==================================================")
print("\nInformasi dataset Ratings:")
ratings.info()

# Menampilkan informasi struktur dataset Users
print("==================================================")
print("\nInformasi dataset Users:")
users.info()

"""## Univariate Exploratory Data Analysis

#### Univariate EDA untuk dataset Books
"""

# Cek data yang hilang
print("\nJumlah data kosong di dataset Books:")
print(books.isnull().sum())

# Cek data duplikat
print("\nJumlah data duplikat di dataset Books:")
print(books.duplicated().sum())

# Visualisasi 10 Penulis Buku Terbanyak
top_authors = books['Book-Author'].value_counts().head(10)

plt.figure(figsize=(10, 6))
barplot = sns.barplot(
    x=top_authors.values,
    y=top_authors.index,
    color='darkblue'
)

for i, value in enumerate(top_authors.values):
    plt.text(value + 0.5, i, str(value), va='center', fontsize=10)

plt.title('10 Penulis dengan Buku Terbanyak')
plt.xlabel('Jumlah Buku')
plt.ylabel('Penulis')
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:

- Agatha Christie adalah penulis dengan jumlah buku terbanyak di antara 10 penulis yang ditampilkan, yaitu sebesar 632 buku. Jumlah ini secara signifikan lebih banyak dibandingkan penulis lainnya dalam daftar.
- Charles Dickens memiliki jumlah buku paling sedikit di antara 10 penulis ini, yaitu sebesar 302 buku.

"""

# Visualisasi 10 penerbit terbanyak
top_publisher = books['Publisher'].value_counts().head(10)

plt.figure(figsize=(10, 6))
barplot = sns.barplot(
    x=top_publisher.values,
    y=top_publisher.index,
    color='darkblue'
)

for i, value in enumerate(top_publisher.values):
    plt.text(value + 0.5, i, str(value), va='center', fontsize=10)

plt.title('10 Penerbit Buku Terbanyak')
plt.ylabel('Nama Penerbit')
plt.xlabel('Jumlah Buku')
plt.xticks(rotation=45)
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:

- Harlequin adalah penerbit dengan jumlah buku terbanyak di antara 10 penerbit yang ditampilkan, yaitu sebesar 7535 buku. Jumlah ini jauh lebih banyak dibandingkan penerbit lainnya dalam daftar.
- Penerbit Warner Books memiliki jumlah buku paling sedikit di antara 10 penerbit ini, yaitu sebesar 2727 buku.

"""

# Visualisasi Panjang Judul Buku (dalam karakter)
plt.figure(figsize=(10, 6))
sns.histplot(books['Book-Title'].apply(len), bins=30, kde=True, color='darkblue')
plt.title('Distribusi Panjang Judul Buku')
plt.xlabel('Panjang Judul (dalam karakter)')
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- Sebagian besar judul buku memiliki panjang yang relatif pendek. Puncak kurva KDE dan batang-batang tertinggi berada di sekitar panjang judul antara 10 hingga 30 karakter.
- Distribusi panjang judul buku miring ke kanan (positively skewed). Hal ini berarti sebagian besar data terkumpul di sisi kiri (judul pendek), dan ada "ekor" yang memanjang ke kanan yang menunjukkan adanya sejumlah kecil judul buku yang sangat panjang.
- Frekuensi buku yang panjang judulnya di atas 100 karakter sangat rendah dan mengalami penurunan seiring bertambahnya panjang.

#### Univariate EDA untuk dataset Ratings
"""

# Cek data yang hilang
print("\nJumlah data kosong di dataset Ratings:")
print(ratings.isnull().sum())

# Cek data duplikat
print("\nJumlah data duplikat di dataset Ratings:")
print(ratings.duplicated().sum())

# Visualisasi distribusi rating buku
plt.figure(figsize=(10, 6))
ax = sns.countplot(x='Book-Rating', data=ratings, color='darkblue')

for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2, height + 5, str(height), ha='center', va='bottom', fontsize=9)

plt.title('Distribusi Rating Buku')
plt.xlabel('Rating Buku')
plt.ylabel('Jumlah')
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- Rating 0 memiliki jumlah yang sangat dominan dibandingkan dengan rating lainnya. Terdapat 716.109 rating dengan nilai 0.
- Rating 1 memiliki jumlah yang paling sedikit, yaitu sebesar 1770 rating.
- Distribusi rating tidak merata. Ada konsentrasi besar pada rating 0, diikuti oleh peningkatan jumlah rating hingga nilai 8, dan kemudian sedikit penurunan pada nilai 9 dan 10.

"""

# Visualisai 10 User Paling Aktif Memberi Rating
top_users = ratings['User-ID'].value_counts().head(10)
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=top_users.index.astype(str), y=top_users.values, color='darkblue')

for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2, height + 5, str(height), ha='center', va='bottom', fontsize=9)

plt.title('Top 10 User Paling Aktif Memberi Rating')
plt.xlabel('User ID')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- User dengan ID 11676 adalah user yang paling aktif dalam memberikan rating, dengan jumlah rating mencapai 13602. Jumlah ini jauh lebih tinggi dibandingkan user aktif lainnya dalam grafik.
- Aktivitas rating menurun secara signifikan setelah user pertama.
- User dengan ID 235105 adalah user yang memberikan rating paling sedikit pada grafik di atas, yaitu sebesar 3067.
"""

# Top 10 Buku dengan rating terbanyak
top_books = ratings['ISBN'].value_counts().head(10)

plt.figure(figsize=(12, 6))
ax = sns.barplot(x=top_books.index, y=top_books.values, color='darkblue')

for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2, height + 5, str(height), ha='center', va='bottom', fontsize=9)

plt.title('Top 10 Buku Paling Banyak Dirating')
plt.xlabel('ISBN')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- Buku dengan ISBN 0971880107 adalah buku yang paling banyak dirating, dengan jumlah rating mencapai 2502. Jumlah ini jauh lebih tinggi dibandingkan buku-buku lain dalam grafik.
- Buku dengan ISBN 0671027360 adalah buku kesepuluh yang paling banyak dirating, dengan jumlah rating 586.

#### Univariate EDA untuk dataset Users
"""

# Cek data yang hilang
print("\nJumlah data kosong di dataset Users:")
print(users.isnull().sum())

# Cek data duplikat
print("\nJumlah data duplikat di dataset Users:")
print(users.duplicated().sum())

# Hitung persentase missing values
nan_data = users['Age'].isna().sum()
total_data = users.shape[0]
persentase_nan = (nan_data / total_data) * 100
print(f"Persentase missing values pada kolom Age adalah: {persentase_nan:.2f}%")

# Menampilkan semua nilai unik pada kolom 'Age'
unique_ages_count = users['Age'].value_counts().sort_index()
pd.set_option('display.max_rows', None)
print("Nilai unik dan jumlah masing-masing pada kolom 'Age':")
print(unique_ages_count)

# Visualisasi distribusi umur pengguna
plt.figure(figsize=(10,6))
sns.histplot(users['Age'].dropna(), bins=30, kde=True, color='darkblue')
plt.title('Distribusi Umur Pengguna')
plt.xlabel('Umur')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- Distribusi umur pengguna miring ke kanan (positively skewed). Ini berarti sebagian besar data terkumpul di sisi kiri (umur muda), dan ada "ekor" yang memanjang ke kanan yang menunjukkan adanya sejumlah kecil pengguna dengan umur yang lebih tua.
- Sebagian besar pengguna berada dalam rentang umur dewasa muda hingga dewasa. Puncak kurva KDE dan batang-batang tertinggi berada di sekitar umur 20 hingga 40 tahun.
- Jumlah pengguna menurun seiring bertambahnya umur setelah rentang 40 tahun. Frekuensi pengguna dengan umur di atas 60 tahun semakin rendah.
- Terdapat beberapa lonjakan kecil pada umur yang tidak lazim (sekitar 100 sampai dengan 150 tahun). Lonjakan ini kemungkinan merupakan kesalahan input data atau nilai ekstrem yang perlu diselidiki lebih lanjut.
"""

# Visualisasi 10 Lokasi dengan Jumlah Pengguna Terbanyak
top_locations = users['Location'].value_counts().head(10)

plt.figure(figsize=(12, 6))
ax = top_locations.plot(kind='bar', color='darkblue')
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2, height + 5, str(height), ha='center', va='bottom', fontsize=9)

plt.title('10 Lokasi dengan Jumlah Pengguna Terbanyak')
plt.xlabel('Lokasi')
plt.ylabel('Jumlah Pengguna')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- London, England, United Kingdom adalah lokasi dengan jumlah pengguna terbanyak, yaitu sebesar 2506 pengguna. Jumlah ini secara signifikan lebih tinggi dibandingkan lokasi lainnya dalam grafik.
- Vancouver, British Columbia, Canada memiliki jumlah pengguna paling sedikit di antara 10 lokasi teratas, yaitu sebesar 1359 pengguna.
"""

# Visualisasi 10 Negara dengan Jumlah Pengguna Terbanyak
users['Negara'] = users['Location'].apply(lambda x: x.strip().split(',')[-1].strip())
negara_teratas = users['Negara'].value_counts().head(10)

plt.figure(figsize=(12, 6))
bars = plt.bar(negara_teratas.index, negara_teratas.values, color='darkblue')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom', fontsize=9)

plt.title('10 Negara dengan Jumlah Pengguna Terbanyak')
plt.xlabel('Negara')
plt.ylabel('Jumlah Pengguna')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- USA (Amerika Serikat) adalah negara dengan jumlah pengguna terbanyak, yaitu sebesar 139711 pengguna. Jumlah ini jauh lebih tinggi dibandingkan negara-negara lain dalam daftar.
-  Portugal memiliki jumlah pengguna yang paling sedikit di antara 10 negara teratas, yaitu sebesar 3325 pengguna serta penurunan yang cukup tajam dibandingkan negara-negara sebelumnya.
"""

# Visualisasi Lokasi dalam Bentuk Map
country_counts = users['Negara'].value_counts().reset_index()
country_counts.columns = ['Country', 'UserCount']

fig = px.choropleth(country_counts,
                    locations='Country',
                    locationmode='country names',
                    color='UserCount',
                    hover_name='Country',
                    color_continuous_scale='plasma',
                    title='Distribusi Pengguna berdasarkan Negara')
fig.show()

"""## Data Preprocessing"""

books_ratings=books.merge(ratings,on="ISBN")
books_ratings.head()

"""## Data Preparation"""

# Copy dataset
df = books_ratings.copy()

# Clean book titles
df["Book-Title"] = df["Book-Title"].apply(lambda x: re.sub("[\W_]+", " ", x).strip())

# Drop irrelevant columns
df.drop(columns=["ISBN", "Image-URL-S", "Image-URL-M", "Image-URL-L"], inplace=True)

# Drop ratings == 0 and missing values
df.drop(index=df[df["Book-Rating"]==0].index, inplace=True)
df.dropna(inplace=True)

# Reset index
df.reset_index(drop=True, inplace=True)


## CONTENT-BASED FILTERING PREPARATION
# Remove duplicates
cbf_df = df.drop_duplicates('Book-Title')

# Sampling max 10,000 books
cbf_df = cbf_df.sample(10000, random_state=42).reset_index(drop=True)

# TF-IDF vectorization
cbf_df['content'] = cbf_df['Book-Title'] + ' ' + cbf_df['Book-Author'] + ' ' + cbf_df['Publisher']
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)
tfidf_matrix = tfidf.fit_transform(cbf_df['content'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
indices = pd.Series(cbf_df.index, index=cbf_df['Book-Title']).drop_duplicates()

## COLLABORATIVE FILTERING PREPARATION
# Filter books/users with >=15 ratings
book_counts = df['Book-Title'].value_counts()
user_counts = df['User-ID'].value_counts()
filtered_books = book_counts[book_counts >= 15].index
filtered_users = user_counts[user_counts >= 15].index
cf_df = df[(df['Book-Title'].isin(filtered_books)) & (df['User-ID'].isin(filtered_users))]

# Mapping to index
user_ids = cf_df['User-ID'].unique().tolist()
book_ids = cf_df['Book-Title'].unique().tolist()
user_to_index = {user: i for i, user in enumerate(user_ids)}
book_to_index = {book: i for i, book in enumerate(book_ids)}
cf_df['user_index'] = cf_df['User-ID'].map(user_to_index)
cf_df['book_index'] = cf_df['Book-Title'].map(book_to_index)

# Split train/test
train_data, test_data = train_test_split(cf_df, test_size=0.2, random_state=42)

"""## Modeling and Result

### Model Development dengan Content Based Filtering
"""

def content_based_recommendations(title, top_n=10):
    if title not in indices:
        return f"Buku '{title}' tidak ditemukan dalam database."
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    book_indices = [i[0] for i in sim_scores]
    result = cbf_df.iloc[book_indices][['Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']]
    result['Similarity Score'] = [i[1] for i in sim_scores]
    return result

"""#### Example of Using Content-Based Filtering Recommendations"""

# Contoh rekomendasi content-based filtering
def interactive_content_based_recommendation(indices, recommendation_func, fallback=True):
    book_title = input("Masukkan judul buku: ").strip()

    if book_title in indices:
        print(f"\nRekomendasi untuk buku '{book_title}':")
        print(recommendation_func(book_title))
    elif fallback:
        sample_title = indices.index[0]
        print(f"\nBuku '{book_title}' tidak ditemukan dalam database.")
        print(f"Berikut adalah rekomendasi untuk buku '{sample_title}':")
        print(recommendation_func(sample_title))
    else:
        print(f"\nBuku '{book_title}' tidak ditemukan dalam database dan fallback dinonaktifkan.")

interactive_content_based_recommendation(indices, content_based_recommendations)

interactive_content_based_recommendation(indices, content_based_recommendations)

"""### Model Development dengan Collaborative Filtering"""

embedding_size = 32

# Input layers
user_input = Input(shape=(1,), name='user_input')
book_input = Input(shape=(1,), name='book_input')

# Embedding layers
user_embedding = Embedding(input_dim=len(user_ids), output_dim=embedding_size)(user_input)
book_embedding = Embedding(input_dim=len(book_ids), output_dim=embedding_size)(book_input)

# Flatten
user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

# Menggabungkan user_vec dan book_vec
concat = tf.keras.layers.Concatenate()([user_vec, book_vec])

# Menggabungkan dan Dense with Dropout
x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(concat)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)

x = Dense(32, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='relu')(x)  # sigmoid untuk target yang dinormalisasi

# Model
model = Model(inputs=[user_input, book_input], outputs=output)
model.compile(optimizer=AdamW(learning_rate=0.0005), loss='mse', metrics=['mae'])

# Normalized targets
X_train = [train_data['user_index'].values, train_data['book_index'].values]
y_train = train_data['Book-Rating'].values / 10.0

X_test = [test_data['user_index'].values, test_data['book_index'].values]
y_test = test_data['Book-Rating'].values / 10.0

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)

# Train
history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=256,
    epochs=30,
    validation_data=(X_test, y_test),
    callbacks=[early_stop],
    verbose=1
)

# Plot Loss and MAE
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Val Loss', color='orange')
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE', color='green')
plt.plot(history.history['val_mae'], label='Val MAE', color='red')
plt.title('MAE Curve')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.legend()

plt.tight_layout()
plt.show()

"""Berdasarkan kedua kurva di atas, dapat menyimpulkan bahwa model tidak mengalami overfitting atau underfitting dan performa pada validation cukup stabil dengan error yang kecil."""

def get_collaborative_recommendations(user_id, top_n=10):
    if user_id not in user_to_index:
        return "User tidak ditemukan"

    user_idx = user_to_index[user_id]
    all_books = np.array(list(book_to_index.values()))
    user_array = np.full_like(all_books, user_idx)

    predictions = model.predict([user_array, all_books], verbose=0).flatten()
    top_indices = predictions.argsort()[::-1][:top_n]

    recommended_books = [book_ids[i] for i in all_books[top_indices]]
    return recommended_books

example_user = cf_df['User-ID'].iloc[200]  # ambil salah satu user dari data
recommendations = get_collaborative_recommendations(example_user, top_n=5)
print(f"Rekomendasi buku untuk User {example_user}:")
for i, book in enumerate(recommendations, 1):
    print(f"{i}. {book}")

def interactive_collaborative_recommendation():
    while True:
        user_id_input = input("Masukkan User-ID (atau ketik 'exit' untuk keluar): ")
        if user_id_input.lower() == 'exit':
            break
        try:
            user_id = int(user_id_input)
            recommendations = get_collaborative_recommendations(user_id, top_n=5)
            if isinstance(recommendations, str):
                print(recommendations)
            else:
                print(f"\nRekomendasi buku untuk User {user_id}:")
                for i, book in enumerate(recommendations, 1):
                    print(f"{i}. {book}")
        except ValueError:
            print("Input tidak valid. Harap masukkan User-ID berupa angka.")

interactive_collaborative_recommendation()

"""## Evaluation

### Evaluasi Content-Based Filtering
"""

# Metrik Precision@k
def evaluate_content_based_precision_at_k(test_data, indices, k=10):
    unique_users = test_data['User-ID'].unique()

    # Batasi jumlah user untuk efisiensi
    if len(unique_users) > 100:
        np.random.seed(42)
        unique_users = np.random.choice(unique_users, 100, replace=False)

    total_precision = 0
    user_count = 0

    for user_id in unique_users:
        liked_books = test_data[(test_data['User-ID'] == user_id) &
                               (test_data['Book-Rating'] >= 8)]['Book-Title'].unique()

        if len(liked_books) < 2:
            continue

        # Filter untuk memastikan buku ada di indices
        liked_books_in_indices = [book for book in liked_books if book in indices]
        if not liked_books_in_indices:
            continue

        user_count += 1
        input_book = liked_books_in_indices[0]

        # Cari rekomendasi berdasarkan buku input
        try:
            recommendations = content_based_recommendations(input_book, top_n=k)
            if isinstance(recommendations, str):
                continue

            # Cek berapa rekomendasi yang ada di liked_books
            recommended_titles = recommendations['Book-Title'].values
            relevant_recommendations = sum(1 for title in recommended_titles if title in liked_books[1:])

            # Hitung precision@k
            precision = relevant_recommendations / k
            total_precision += precision
        except Exception as e:
            user_count -= 1
            continue

    if user_count == 0:
        return 0
    return total_precision / user_count

# Metrik Coverage
def evaluate_content_based_coverage(cbf_df, indices):
    all_books = cbf_df['Book-Title'].unique()
    total_books = len(all_books)

    # Ambil sample buku untuk testing (lebih kecil untuk efisiensi)
    np.random.seed(42)
    sample_size = min(20, total_books)
    sampled_books = np.random.choice(all_books, sample_size, replace=False)

    # Filter untuk memastikan buku ada di indices
    sampled_books_in_indices = [book for book in sampled_books if book in indices]

    recommended_books = set()

    for book in sampled_books_in_indices:
        try:
            recommendations = content_based_recommendations(book, top_n=10)
            if isinstance(recommendations, str):
                continue

            # Tambahkan buku yang direkomendasikan ke set
            recommended_books.update(recommendations['Book-Title'].values)
        except Exception as e:
            continue

    coverage = len(recommended_books) / total_books
    return coverage

# Metrik Diversity
def evaluate_content_based_diversity(cbf_df, indices, cosine_sim):
    all_books = cbf_df['Book-Title'].unique()

    # Sample buku untuk testing
    np.random.seed(42)
    sample_size = min(20, len(all_books))
    sampled_books = np.random.choice(all_books, sample_size, replace=False)

    # Filter untuk memastikan buku ada di indices
    sampled_books_in_indices = [book for book in sampled_books if book in indices]

    total_diversity = 0
    count = 0

    for book in sampled_books_in_indices:
        try:
            recommendations = content_based_recommendations(book, top_n=10)
            if isinstance(recommendations, str):
                continue

            recommended_titles = recommendations['Book-Title'].values
            recommended_indices = [indices[title] for title in recommended_titles if title in indices]

            if len(recommended_indices) < 2:
                continue

            # Hitung rata-rata similarity antar buku yang direkomendasikan
            sum_similarity = 0
            pairs = 0

            for i in range(len(recommended_indices)):
                for j in range(i+1, len(recommended_indices)):
                    sum_similarity += cosine_sim[recommended_indices[i]][recommended_indices[j]]
                    pairs += 1

            if pairs > 0:
                avg_similarity = sum_similarity / pairs
                diversity = 1 - avg_similarity
                total_diversity += diversity
                count += 1
        except Exception as e:
            continue

    if count == 0:
        return 0
    return total_diversity / count

# Evaluasi model Content-Based Filtering
def evaluate_content_based_model(test_data, cbf_df, indices, cosine_sim):
    precision_at_k = evaluate_content_based_precision_at_k(test_data, indices)
    coverage = evaluate_content_based_coverage(cbf_df, indices)
    diversity = evaluate_content_based_diversity(cbf_df, indices, cosine_sim)

    print("\nEvaluasi Model Content-Based Filtering:")
    print(f"Precision@10: {precision_at_k:.4f}")
    print(f"Coverage: {coverage:.4f}")
    print(f"Diversity: {diversity:.4f}")

    return precision_at_k, coverage, diversity

precision_at_k, coverage, diversity_cbf = evaluate_content_based_model(test_data, cbf_df, indices, cosine_sim)

"""Berikut penjelasan dari metrik evaluasi Content-Based Filtering:

1. Precision@10 : Dari 10 rekomendasi yang diberikan ke pengguna, rata-rata hanya 1.43% (0.0143) yang benar-benar relevan atau disukai pengguna karena sistem rekomendasi memberikan banyak rekomendasi yang tidak sesuai preferensi pengguna.

2. Coverage : Sistem hanya merekomendasikan 1.98% (0.0198)dari seluruh item yang tersedia. Hal ini menunjukkan bahwa ruang rekomendasi sangat sempit.

3. Diversity : Berdasarkan output 0.7468 (74.68%), hal ini menunjukkan bahwa rekomendasi cukup beragam  satu sama lain, artinya model merekomendasikan item yang relatif tidak terlalu mirip satu sama lain (dari segi konten).
"""

def visualize_cbf_metrics(precision_at_k, coverage, diversity):
    metrics = ['Precision@10', 'Coverage', 'Diversity']
    values = [precision_at_k, coverage, diversity]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics, values, color=['red', 'green', 'blue'])

    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.4f}', ha='center', va='bottom', fontsize=10)

    plt.title('Metrik Evaluasi untuk Content-Based Filtering')
    plt.ylim(0, 1)
    plt.ylabel('Skor')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

visualize_cbf_metrics(precision_at_k, coverage, diversity_cbf)

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:


- Precision@10 : Nilai presisi pada 10 rekomendasi teratas adalah 0.0143. Nilai ini relatif rendah, yang mengindikasikan bahwa hanya sebagian kecil (sekitar 1.43%) dari 10 item teratas yang direkomendasikan dianggap relevan oleh pengguna.

- Coverage : Nilai coverage adalah 0.0198. Nilai ini juga sangat rendah, yang berarti sistem rekomendasi berbasis konten ini hanya mampu merekomendasikan sebagian kecil (sekitar 1.98%) dari keseluruhan item yang tersedia dalam katalog.

- Diversity : Nilai diversitas adalah 0.7468. Nilai ini cukup tinggi, menunjukkan bahwa rekomendasi yang diberikan oleh sistem ini cenderung sangat beragam.

Berdasarkan metrik evaluasi ini, sistem rekomendasi berbasis konten yang dievaluasi menunjukkan performa yang kurang memuaskan dalam hal relevansi (Precision@10) dan jangkauan (Coverage), meskipun memiliki tingkat diversitas yang cukup tinggi.

### Evaluasi Collaborative Filtering
"""

def evaluate_collaborative_filtering(model, test_data, user_ids, book_ids, user_to_index, book_to_index):
    # Ekstrak fitur test dan normalisasi target
    X_test_user = test_data['user_index'].values
    X_test_book = test_data['book_index'].values
    y_test = test_data['Book-Rating'].values / 10.0  # Normalisasi

    max_rating = 10.0  # Rating maksimum

    # Prediksi dan evaluasi
    y_pred = model.predict([X_test_user, X_test_book], verbose=0)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)

    # Konversi ke skala asli
    rmse_actual = rmse * max_rating
    mae_actual = mae * max_rating

    # Fungsi helper untuk mendapatkan rekomendasi collaborative filtering
    def get_collaborative_recommendations_for_eval(user_id, top_n=10):
        if user_id not in user_to_index:
            return "User tidak ditemukan"

        user_idx = user_to_index[user_id]
        all_books = np.array(list(range(len(book_ids))))
        user_array = np.full_like(all_books, user_idx)

        predictions = model.predict([user_array, all_books], verbose=0).flatten()
        top_indices = predictions.argsort()[::-1][:top_n]

        recommended_books = [book_ids[i] for i in top_indices]
        return pd.DataFrame({'Book-Title': recommended_books})

    # Evaluasi Recall@k
    def evaluate_recall_at_k(model, test_data, k=10, threshold=0.6):
        test_users = test_data['User-ID'].unique()

        # Batasi jumlah user untuk efisiensi
        if len(test_users) > 100:
            np.random.seed(42)
            test_users = np.random.choice(test_users, 100, replace=False)

        total_recall = 0
        user_count = 0

        for user_id in test_users:
            # Buku yang disukai pengguna (dengan rating >= threshold * 10)
            liked_books = test_data[(test_data['User-ID'] == user_id) &
                                  (test_data['Book-Rating'] >= threshold * 10)]['Book-Title'].values
            if len(liked_books) == 0:
                continue
            user_count += 1

            try:
                recommendations = get_collaborative_recommendations_for_eval(user_id, top_n=k)
                if isinstance(recommendations, str):
                    continue

                # Cek berapa rekomendasi yang ada di liked_books
                recommended_titles = recommendations['Book-Title'].values
                relevant_recommendations = sum(1 for title in recommended_titles if title in liked_books)

                # Hitung recall@k
                recall = relevant_recommendations / len(liked_books) if len(liked_books) > 0 else 0
                total_recall += recall
            except Exception as e:
                user_count -= 1
                continue

        if user_count == 0:
            return 0
        return total_recall / user_count

    # Evaluasi Hit Rate
    def evaluate_hit_rate(model, test_data, k=10):
        test_users = test_data['User-ID'].unique()

        # Batasi jumlah user untuk efisiensi
        if len(test_users) > 100:
            np.random.seed(42)
            test_users = np.random.choice(test_users, 100, replace=False)

        hits = 0
        valid_users = 0

        for user_id in test_users:
            user_books = test_data[test_data['User-ID'] == user_id]['Book-Title'].values
            if len(user_books) == 0:
                continue

            valid_users += 1
            try:
                recommendations = get_collaborative_recommendations_for_eval(user_id, top_n=k)

                if isinstance(recommendations, str):
                    continue

                # Cek apakah ada rekomendasi yang ada di buku user
                recommended_titles = recommendations['Book-Title'].values

                if any(title in user_books for title in recommended_titles):
                    hits += 1
            except Exception as e:
                valid_users -= 1
                continue

        if valid_users == 0:
            return 0
        hit_rate = hits / valid_users
        return hit_rate

    recall_at_10 = evaluate_recall_at_k(model, test_data, k=10)
    hit_rate = evaluate_hit_rate(model, test_data, k=10)

    print("\nEvaluasi Model Collaborative Filtering:")
    print(f"RMSE (skala 0-1): {rmse:.4f}")
    print(f"MAE (skala 0-1): {mae:.4f}")
    print(f"RMSE (skala asli 1-10): {rmse_actual:.4f}")
    print(f"MAE (skala asli 1-10): {mae_actual:.4f}")
    print(f"Recall@10: {recall_at_10:.4f}")
    print(f"Hit Rate: {hit_rate:.4f}")

    return rmse, mae, rmse_actual, mae_actual, recall_at_10, hit_rate

def visualize_cf_metrics(rmse_actual, mae_actual, recall_at_10, hit_rate):
    metrics_cf = ['RMSE/10', 'MAE/10', 'Recall@10', 'Hit Rate']
    values_cf = [rmse_actual/10, mae_actual/10, recall_at_10, hit_rate]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics_cf, values_cf, color=['red', 'blue', 'green', 'purple'])

    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.4f}', ha='center', va='bottom', fontsize=10)

    plt.title('Metrik Evaluasi untuk Collaborative Filtering')
    plt.ylim(0, 1)
    plt.ylabel('Skor')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

rmse, mae, rmse_actual, mae_actual, recall_at_10, hit_rate = evaluate_collaborative_filtering(
        model, test_data, user_ids, book_ids, user_to_index, book_to_index)

"""Berikut penjelasan dari metrik evaluasi Collaborative Filtering:

- RMSE mengukur seberapa besar kesalahan rata-rata antara prediksi rating dan rating sebenarnya, dengan memberi penalti lebih besar untuk kesalahan besar. Nilai 1.5983 berarti, rata-rata prediksi rating meleset sekitar 1.6 poin dari rating sebenarnya.

- MAE mengukur rata-rata kesalahan absolut antara rating yang diprediksi dan rating aktual tanpa penalti berlebih untuk outlier, berbeda dengan RMSE. Nilai 1.25 berarti rata-rata prediksi meleset sekitar 1.25 poin dari rating asli.

- Recall@10 menunjukkan seberapa banyak item relevan (yang benar-benar disukai pengguna) berhasil muncul di 10 rekomendasi teratas. Nilai 0.0044 berarti hanya 0.44% dari item relevan yang berhasil direkomendasikan.

- Hit Rate menunjukkan seberapa sering setidaknya satu item yang direkomendasikan muncul di daftar item yang benar-benar relevan bagi pengguna. Nilai 0.02 berarti hanya 2% dari pengguna yang mendapat rekomendasi yang sesuai (hit), sisanya yaitu 98% dari pengguna tidak mendapatkan rekomendasi yang cocok sama sekali dari sistem.
"""

visualize_cf_metrics(rmse_actual, mae_actual, recall_at_10, hit_rate)

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa sistem rekomendasi Collaborative Filtering ini menunjukkan performa yang kurang baik, terutama dalam hal kemampuan menemukan item yang relevan (Recall@10 dan Hit Rate). Meskipun nilai kesalahan prediksi (RMSE@10 dan MAE@10) relatif rendah, kemampuan sistem untuk benar-benar merekomendasikan item yang relevan bagi pengguna dalam daftar teratas sangat terbatas.

### Perbandingan Model
"""

# Visualisasi perbandingan antara kedua model
def compare_models(precision_at_k, recall_at_10, diversity_cbf, hit_rate):
    fig, ax = plt.subplots(figsize=(12, 7))

    models = ['Content-Based Filtering', 'Collaborative Filtering']
    precision_recall = [precision_at_k, recall_at_10]
    interaction_metrics = [diversity_cbf, hit_rate]
    x = np.arange(len(models))
    width = 0.35

    bars1 = ax.bar(x - width/2, precision_recall, width, label='Precision@k / Recall@k')
    bars2 = ax.bar(x + width/2, interaction_metrics, width, label='Diversity / Hit Rate')

    for bar in bars1 + bars2:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            height + 0.02,
            f'{height:.4f}',
            ha='center',
            va='bottom',
            fontsize=10,
            color='black'
        )

    ax.set_ylabel('Skor')
    ax.set_title('Perbandingan Metrik Evaluasi Antar Model')
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.legend()
    ax.set_ylim(0, 1.1)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

compare_models(precision_at_k, recall_at_10, diversity_cbf, hit_rate)

"""Berdasarkan visualisasi di atas, dapat disimpulkan bahwa:
- Kedua model, baik Content-Based Filtering maupun Collaborative Filtering, menunjukkan performa yang sangat rendah dalam hal merekomendasikan item yang relevan (berdasarkan metrik Precision@k / Recall@k).

- Model Content-Based Filtering unggul jauh dalam hal diversitas rekomendasi dibandingkan dengan Collaborative Filtering. Namun, keberagaman tanpa relevansi mungkin tidak memberikan pengalaman pengguna yang baik.

- Model Collaborative Filtering menunjukkan performa yang buruk.

Secara keseluruhan, visualisasi ini memberikan gambaran yang jelas bahwa perlu ada upaya yang signifikan untuk meningkatkan kualitas sistem rekomendasi yang ada, terlepas dari pendekatan (content-based atau collaborative) yang digunakan.

Berikut analis kelebihan dan kekurangan kedua model:

1. Content-Based Filtering

  - Kelebihan:
    1.  Memberikan rekomendasi yang bervariasi, tidak terjebak pada item-item populer saja.
    2. Bisa memberi rekomendasi meski hanya berdasarkan fitur item + histori pengguna (cold-start user sedikit lebih mudah diatasi).
    3. Lebih bisa dikontrol.

  - Kekurangan:
    1.  Hanya 1.4% dari top 10 rekomendasi yang benar-benar relevan.
    2. Model hanya menggunakan 1.98% dari seluruh item dalam rekomendasi, artinya item yang direkomendasikan itu-itu saja, dan kurang menjangkau seluruh inventori.
    3. Tergantung kualitas fitur item.

1. Collaborative Filtering

  - Kelebihan:
    1.  Mengandalkan kemiripan antar pengguna/item sehingga hasil lebih spesifik untuk individu.
    2. Tidak butuh fitur item karena cukup berdasarkan user-item interaction matrix.
    3. Dapat menangkap pola selera kompleks yang tidak terlihat dari fitur item saja.

  - Kekurangan:
    1.   Hanya 0.44% dari item relevan muncul dalam top 10, artinya rekomendasi banyak yang tidak sesuai.
    2. Hanya 2% pengguna yang menerima rekomendasi relevan sama sekali.
    3. RMSE 1.59 dan MAE 1.25 menunjukkan prediksi rating belum akurat.
    4. Sulit bekerja jika pengguna atau item baru belum memiliki cukup interaksi.

Berdasarkan kelemahan dan kelebihan model di atas, rekomendasi model terbaik Content-Based Filtering karena menghasilkan rekomendasi yang lebih relevan, bervariasi, dan stabil, terutama jika data interaksi pengguna masih terbatas. Collaborative Filtering masih perlu peningkatan signifikan dalam kualitas prediksi dan recall.
"""

